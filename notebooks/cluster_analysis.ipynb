{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Imports, Functions, & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "from glob import glob\n",
    "from zipfile import ZipFile\n",
    "import shutil\n",
    "\n",
    "# Clustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Imputing missing values\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif, RFE\n",
    "\n",
    "# from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove outliers based on IQR method\n",
    "def remove_outliers(df):\n",
    "    # Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.75)\n",
    "    \n",
    "    # Calculate the IQR\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Define outlier bounds (1.5 * IQR rule)\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Remove outliers\n",
    "    df_no_outliers = df[~((df < lower_bound) | (df > upper_bound)).any(axis=1)]\n",
    "    \n",
    "    return df_no_outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_healthy_mci_dementia_kmeans_knn_dbscan(fALFF_DMN_df_scaled, eps):\n",
    "    \"\"\"\n",
    "    This function will attempt to find 3 clusters in the input df.\n",
    "    The clusters correspond to healthy, mci, and demented levels \n",
    "    of cognition.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fALFF_DMN_df_scaled : pandas dataframe\n",
    "        This is the pandas dataframe containing scaled data to be plot.\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(n_clusters=3)\n",
    "    kmeans.fit(fALFF_DMN_df_scaled)\n",
    "    pred_label = kmeans.predict(fALFF_DMN_df_scaled)\n",
    "    score = silhouette_score(fALFF_DMN_df_scaled, kmeans.labels_, random_state=42)\n",
    "    print(\"Silhouette Score:\", score)\n",
    "    print(f\"Predicted Labels: {pred_label}\")\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(fALFF_DMN_df_scaled)\n",
    "\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=pred_label, cmap='viridis')\n",
    "    plt.title(\"KMeans Clusters (PCA Reduced Data) Default Mode Network fALFF\")\n",
    "    plt.show()\n",
    "\n",
    "    # Find distances to the k-th nearest neighbor (here k = min_samples)\n",
    "    min_samples = 10  # minimum number of points required to form a cluster\n",
    "    k = min_samples\n",
    "    neighbors = NearestNeighbors(n_neighbors=k)\n",
    "    neighbors_fit = neighbors.fit(X_pca)\n",
    "    distances, indices = neighbors_fit.kneighbors(X_pca)\n",
    "\n",
    "\n",
    "    # Sort the distances for plotting\n",
    "    distances = np.sort(distances[:, k-1], axis=0)\n",
    "\n",
    "    # Plot the k-distance graph\n",
    "    plt.plot(distances)\n",
    "    plt.title('K-Distance Graph')\n",
    "    plt.xlabel('Points sorted by distance')\n",
    "    plt.ylabel(f'Distance to {k}-th nearest neighbor')\n",
    "    # Find the optimal eps by identifying the elbow point\n",
    "    # The elbow can be detected by finding the point where the distance starts to increase rapidly\n",
    "    # We'll find the \"elbow\" by looking for the maximum change in slope\n",
    "    gradient = np.diff(distances)  # Calculate the differences between consecutive distances\n",
    "    second_derivative = np.diff(gradient)  # Second derivative approximates the curvature\n",
    "    optimal_eps_index = np.argmax(second_derivative) + 1  # Find the index where second derivative is maximal\n",
    "\n",
    "    # Optimal eps is the distance at this index\n",
    "    optimal_eps = distances[optimal_eps_index]\n",
    "\n",
    "    # Plot horizontal line at the optimal eps value\n",
    "    plt.axhline(y=optimal_eps, color='r', linestyle='--', label=f'Optimal eps = {optimal_eps:.2f}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(f'Optimal eps (elbow point): {optimal_eps:.2f}')\n",
    "\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=3)  # Adjust eps and min_samples\n",
    "    labels = dbscan.fit_predict(fALFF_DMN_df_scaled)\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis')\n",
    "    plt.title(\"DBSCAN Clusters (PCA Reduced Data)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dir = \"../data/dataframes/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the BOLD signal data\n",
    "with ZipFile(df_dir + \"BOLD.zip\") as zfile:\n",
    "    zfile.extractall(df_dir)\n",
    "BOLD_csv_l = glob(df_dir + \"BOLD/\" + \"*.csv\", recursive=True)\n",
    "BOLD_df_l = []\n",
    "for csv in BOLD_csv_l:\n",
    "    BOLD_df_l.append(pd.read_csv(csv))\n",
    "shutil.rmtree(df_dir + \"BOLD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fALFF, CV, & subject names\n",
    "fALFF_df = pd.read_csv(df_dir + \"fALFF.csv\", index_col=0)\n",
    "cv_df = pd.read_csv(df_dir + \"cv_df.csv\", index_col=0)\n",
    "p_df = pd.read_csv(df_dir + \"participant_imputed_cleaned_master_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fALFF_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(BOLD_df_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOLD_df_l[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Participant Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = p_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_df[\"moca_total\"].plot()\n",
    "p_df[\"moca_total\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_df[\"moca_total\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_df[\"global\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_df[\"global\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_df[\"global\"].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-numeric and irrelevant columns if any\n",
    "df = p_df.select_dtypes(include=[np.number])\n",
    "df = df.drop(columns=['sex', 'years_of_education'])\n",
    "# df = df.select_dtypes(include=[np.number])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data_df = pd.DataFrame(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = round(len(scaled_data_df) * .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data before imputation\n",
    "train, test = train_test_split(scaled_data_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the pipeline\n",
    "pipe = Pipeline([('imputer', KNNImputer())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the param_grid \n",
    "param_grid = {'imputer__n_neighbors': range(2, 11)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, scoring='neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on Training Data\n",
    "grid.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best n neighbors\n",
    "best_n = grid.best_params_['imputer__n_neighbors']\n",
    "print(f\"Best n neighbors: {best_n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing the Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute using best model\n",
    "best_imputer = KNNImputer(n_neighbors=best_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data_df_imputed = pd.DataFrame(best_imputer.fit_transform(scaled_data_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(scaled_data_df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(scaled_data_df_imputed.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(scaled_data_df[[5, 16]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(scaled_data_df_imputed[[5, 16]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning & Saving Imputed Dataform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_data = scaler.inverse_transform(scaled_data_df_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_columns = [c for c in p_df.columns if c not in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_data_imputed_df = pd.DataFrame(unscaled_data, columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_imputed_df = pd.concat([p_df[missing_columns], unscaled_data_imputed_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_imputed_df[\"handedness\"] = np.array([int(re.sub(r'[^0-9.]', '',value)) for value in p_imputed_df[\"handedness\"].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_df_imputed_cleaned = p_imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_df_imputed_cleaned.to_csv(\"../data/dataframes/participant_imputed_cleaned_master_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df = pd.DataFrame(scaler.fit_transform(p_df.drop(missing_columns, axis=1)), columns=p_df.drop(missing_columns, axis=1).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply KMeans with 3 clusters\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "labels = kmeans.fit_predict(scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = p_df[\"global\"].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = silhouette_score(scaled_df, kmeans.labels_)\n",
    "print(\"Silhouette Score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximizing Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(scaled_df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_columns_to_drop = [\"executive\", \"attention\", \"memory\", \"global\", \"language\", \"visuospatial\", \"sdmt_oral\", \"cvlt_ld_tot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df_uncorrelated = scaled_df.drop(corr_columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(scaled_df_uncorrelated.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df_uncorrelated_no_outliers = scaled_df_uncorrelated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df_uncorrelated_no_outliers = remove_outliers(scaled_df_uncorrelated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean = KMeans(n_clusters=3)\n",
    "kmeans.fit(scaled_df_uncorrelated_no_outliers)\n",
    "pred_label = kmeans.predict(scaled_df_uncorrelated_no_outliers)\n",
    "score = silhouette_score(scaled_df_uncorrelated_no_outliers, kmeans.labels_, random_state=42)\n",
    "print(\"Silhouette Score:\", score)\n",
    "print(f\"Predicted Labels: {pred_label}\")\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(scaled_df_uncorrelated_no_outliers)\n",
    "\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=pred_label, cmap='viridis')\n",
    "plt.title(\"KMeans Clusters (PCA Reduced Data)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN & DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find distances to the k-th nearest neighbor (here k = min_samples)\n",
    "min_samples = 10  # minimum number of points required to form a cluster\n",
    "k = min_samples\n",
    "neighbors = NearestNeighbors(n_neighbors=k)\n",
    "neighbors_fit = neighbors.fit(X_pca)\n",
    "distances, indices = neighbors_fit.kneighbors(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sort the distances for plotting\n",
    "distances = np.sort(distances[:, k-1], axis=0)\n",
    "\n",
    "# Plot the k-distance graph\n",
    "plt.plot(distances)\n",
    "plt.title('K-Distance Graph')\n",
    "plt.xlabel('Points sorted by distance')\n",
    "plt.ylabel(f'Distance to {k}-th nearest neighbor')\n",
    "# Find the optimal eps by identifying the elbow point\n",
    "# The elbow can be detected by finding the point where the distance starts to increase rapidly\n",
    "# We'll find the \"elbow\" by looking for the maximum change in slope\n",
    "gradient = np.diff(distances)  # Calculate the differences between consecutive distances\n",
    "second_derivative = np.diff(gradient)  # Second derivative approximates the curvature\n",
    "optimal_eps_index = np.argmax(second_derivative) + 1  # Find the index where second derivative is maximal\n",
    "\n",
    "# Optimal eps is the distance at this index\n",
    "optimal_eps = distances[optimal_eps_index]\n",
    "\n",
    "# Plot horizontal line at the optimal eps value\n",
    "plt.axhline(y=optimal_eps, color='r', linestyle='--', label=f'Optimal eps = {optimal_eps:.2f}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f'Optimal eps (elbow point): {optimal_eps:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dbscan = DBSCAN(eps=optimal_eps, min_samples=3)  # Adjust eps and min_samples\n",
    "labels = dbscan.fit_predict(scaled_df_uncorrelated_no_outliers)\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis')\n",
    "plt.title(\"DBSCAN Clusters (PCA Reduced Data)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Clusters Based on fALFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fALFF_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DMN = ['Frontal Pole', 'Precuneous Cortex', 'Angular Gyrus', 'Cingulate Gyrus, anterior division', 'Cingulate Gyrus, posterior division', 'Parahippocampal Gyrus, anterior division', 'Parahippocampal Gyrus, posterior division', 'Frontal Orbital Cortex', 'Left Hippocampus', 'Right Hippocampus', 'Temporal Pole', 'Substantia Nigra Left', 'Substantia Nigra Right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fALFF_df.loc[:, DMN].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fALFF_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(fALFF_df.loc[:, DMN].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fALFF_DMN_df_scaled = pd.DataFrame(scaler.fit_transform(fALFF_df.loc[:, DMN]), columns=fALFF_df.loc[:, DMN].columns)\n",
    "# cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fALFF_DMN_df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Missing Values in fALFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fALFF_df_scaled = scaler.fit_transform(fALFF_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data before imputation\n",
    "# train, test = train_test_split(fALFF_DMN_df_scaled, test_size=0.2, random_state=42)\n",
    "train = fALFF_df_scaled[:train_split, :]\n",
    "test = fALFF_df_scaled[train_split:, :]\n",
    "\n",
    "# Defining the pipeline\n",
    "pipe = Pipeline([('imputer', KNNImputer())])\n",
    "\n",
    "# Defining the param_grid \n",
    "param_grid = {'imputer__n_neighbors': range(2, 11)}\n",
    "\n",
    "# Grid Search\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, scoring='neg_mean_absolute_error')\n",
    "\n",
    "# Fit on Training Data\n",
    "grid.fit(train)\n",
    "\n",
    "# Best n neighbors\n",
    "best_n = grid.best_params_['imputer__n_neighbors']\n",
    "print(f\"Best n neighbors: {best_n}\")\n",
    "\n",
    "# Impute using best model\n",
    "best_imputer = KNNImputer(n_neighbors=best_n)\n",
    "fALFF_df_scaled_imputed = pd.DataFrame(best_imputer.fit_transform(fALFF_df_scaled), columns=fALFF_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fALFF_df_scaled_imputed.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuing Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(fALFF_DMN_df_scaled_imputed)\n",
    "pred_label = kmeans.predict(fALFF_DMN_df_scaled_imputed)\n",
    "score = silhouette_score(fALFF_DMN_df_scaled_imputed, kmeans.labels_, random_state=42)\n",
    "print(\"Silhouette Score:\", score)\n",
    "print(f\"Predicted Labels: {pred_label}\")\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(fALFF_DMN_df_scaled_imputed)\n",
    "\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=pred_label, cmap='viridis')\n",
    "plt.title(\"KMeans Clusters (PCA Reduced Data) Default Mode Network fALFF\")\n",
    "plt.show()\n",
    "\n",
    "### TSNE\n",
    "tsne = TSNE(perplexity=5)\n",
    "X_pca_tsne = tsne.fit_transform(X_pca)\n",
    "\n",
    "plt.scatter(X_pca_tsne[:, 0], X_pca_tsne[:, 1], c=pred_label, cmap='viridis')\n",
    "plt.title(\"KMeans Clusters (PCA Reduced Data) Default Mode Network fALFF\")\n",
    "plt.show()\n",
    "\n",
    "# print(f\"PCA prev_pred_label: {prev_pred_label}\\n\")\n",
    "print(f\"TSNE pred_label: {pred_label}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prev_pred_label = pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(fALFF_DMN_df_scaled_imputed)\n",
    "pred_label = kmeans.predict(fALFF_DMN_df_scaled_imputed)\n",
    "score = silhouette_score(fALFF_DMN_df_scaled_imputed, kmeans.labels_, random_state=42)\n",
    "print(\"Silhouette Score:\", score)\n",
    "print(f\"Predicted Labels: {pred_label}\")\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(fALFF_DMN_df_scaled_imputed)\n",
    "\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=pred_label, cmap='viridis')\n",
    "plt.title(\"KMeans Clusters (PCA Reduced Data) Default Mode Network fALFF\")\n",
    "plt.show()\n",
    "\n",
    "# Find distances to the k-th nearest neighbor (here k = min_samples)\n",
    "min_samples = 10  # minimum number of points required to form a cluster\n",
    "k = min_samples\n",
    "neighbors = NearestNeighbors(n_neighbors=k)\n",
    "neighbors_fit = neighbors.fit(X_pca)\n",
    "distances, indices = neighbors_fit.kneighbors(X_pca)\n",
    "\n",
    "\n",
    "# Sort the distances for plotting\n",
    "distances = np.sort(distances[:, k-1], axis=0)\n",
    "\n",
    "# Plot the k-distance graph\n",
    "plt.plot(distances)\n",
    "plt.title('K-Distance Graph')\n",
    "plt.xlabel('Points sorted by distance')\n",
    "plt.ylabel(f'Distance to {k}-th nearest neighbor')\n",
    "# Find the optimal eps by identifying the elbow point\n",
    "# The elbow can be detected by finding the point where the distance starts to increase rapidly\n",
    "# We'll find the \"elbow\" by looking for the maximum change in slope\n",
    "gradient = np.diff(distances)  # Calculate the differences between consecutive distances\n",
    "second_derivative = np.diff(gradient)  # Second derivative approximates the curvature\n",
    "optimal_eps_index = np.argmax(second_derivative) + 1  # Find the index where second derivative is maximal\n",
    "\n",
    "# Optimal eps is the distance at this index\n",
    "optimal_eps = distances[optimal_eps_index]\n",
    "\n",
    "# Plot horizontal line at the optimal eps value\n",
    "plt.axhline(y=2.0, color='r', linestyle='--', label=f'Optimal eps = {optimal_eps:.2f}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f'Optimal eps (elbow point): {optimal_eps:.2f}')\n",
    "\n",
    "dbscan = DBSCAN(eps=2.0, min_samples=3)  # Adjust eps and min_samples\n",
    "labels = dbscan.fit_predict(fALFF_DMN_df_scaled_imputed)\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis')\n",
    "plt.title(\"DBSCAN Clusters (PCA Reduced Data)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted Labels from the Kmeans clusters of fALFF\n",
    "predicted_labels_healthy_mci_dementia = [1 ,2 ,1 ,1 ,1 ,0 ,1 ,1 ,0 ,0 ,1 ,1 ,1 ,2 ,0 ,2 ,0 ,0 ,0 ,1 ,2 ,0 ,1 ,2 ,2 ,1 ,2 ,0 ,2 ,1 ,2 ,1 ,0 ,1 ,0 ,2 ,1\n",
    " ,0 ,0 ,1 ,1 ,0 ,0 ,1 ,0 ,0 ,0 ,0 ,1 ,0 ,0 ,0 ,1 ,1 ,1 ,1 ,2 ,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_healthy_mci_dementia_kmeans_knn_dbscan(fALFF_DMN_df_scaled_imputed, 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing missing values in cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df_scaled = scaler.fit_transform(cv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df_scaled = pd.DataFrame(cv_df_scaled, columns=cv_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = cv_df_scaled.loc[:train_split, :]\n",
    "test = cv_df_scaled.loc[train_split:, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data before imputation\n",
    "# train, test = train_test_split(cv_df_scaled, test_size=0.2, random_state=42)\n",
    "# Defining the pipeline\n",
    "pipe = Pipeline([('imputer', KNNImputer())])\n",
    "\n",
    "# Defining the param_grid \n",
    "param_grid = {'imputer__n_neighbors': range(2, 11)}\n",
    "\n",
    "# Grid Search\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, scoring='neg_mean_absolute_error')\n",
    "\n",
    "# Fit on Training Data\n",
    "grid.fit(train)\n",
    "\n",
    "# Best n neighbors\n",
    "best_n = grid.best_params_['imputer__n_neighbors']\n",
    "print(f\"Best n neighbors: {best_n}\")\n",
    "\n",
    "# Impute using best model\n",
    "best_imputer = KNNImputer(n_neighbors=best_n)\n",
    "cv_df_scaled_imputed = pd.DataFrame(best_imputer.fit_transform(cv_df_scaled), columns=cv_df_scaled.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df_scaled_imputed.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuing Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_healthy_mci_dementia_kmeans_knn_dbscan(scaler.fit_transform(cv_df_scaled_imputed.loc[:, DMN]), 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted Labels from the CV of the DMN\n",
    "predicted_labels_cv_dmn = [0 ,1 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,2 ,0 ,1 ,0 ,1 ,0 ,0 ,2 ,2 ,1 ,0 ,0 ,1 ,2 ,0 ,1 ,0 ,1 ,2 ,1 ,0 ,2 ,0 ,0 ,1 ,0\n",
    " ,0 ,0 ,0 ,2 ,2 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,2 ,2 ,1 ,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_healthy_mci_dementia.count(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_cv_dmn.count(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_healthy_mci_dementia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_cv_dmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(cv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(fALFF_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fALFF_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fALFF_df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fALFF_df_imputed = pd.DataFrame(scaler.inverse_transform(fALFF_df_scaled_imputed), columns = fALFF_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fALFF_df_imputed.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fALFF_df_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fALFF_df_imputed.to_csv(\"../data/dataframes/fALFF_imputed.csv\", index=False)\n",
    "cv_df_imputed.to_csv(\"../data/dataframes/cv_df_imputed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Thus Far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have a dataset and I don't know what the ground truth for the patients is. I have three known groups, Healthy, MCI, and Demented. I have computed the fALFF and CV of the BOLD regions of the following regions from the fMRI of the patients:\n",
    "\n",
    "['Frontal Pole', 'Insular Cortex', 'Superior Frontal Gyrus',\n",
    "       'Middle Frontal Gyrus',\n",
    "       'Inferior Frontal Gyrus, pars triangularis',\n",
    "       'Inferior Frontal Gyrus, pars opercularis', 'Precentral Gyrus',\n",
    "       'Temporal Pole', 'Superior Temporal Gyrus, anterior division',\n",
    "       'Superior Temporal Gyrus, posterior division',\n",
    "       'Middle Temporal Gyrus, anterior division',\n",
    "       'Middle Temporal Gyrus, posterior division',\n",
    "       'Middle Temporal Gyrus, temporooccipital part',\n",
    "       'Inferior Temporal Gyrus, anterior division',\n",
    "       'Inferior Temporal Gyrus, posterior division',\n",
    "       'Inferior Temporal Gyrus, temporooccipital part',\n",
    "       'Postcentral Gyrus', 'Superior Parietal Lobule',\n",
    "       'Supramarginal Gyrus, anterior division',\n",
    "       'Supramarginal Gyrus, posterior division', 'Angular Gyrus',\n",
    "       'Lateral Occipital Cortex, superior division',\n",
    "       'Lateral Occipital Cortex, inferior division',\n",
    "       'Intracalcarine Cortex', 'Frontal Medial Cortex',\n",
    "       'Juxtapositional Lobule Cortex (formerly Supplementary Motor Cortex)',\n",
    "       'Subcallosal Cortex', 'Paracingulate Gyrus',\n",
    "       'Cingulate Gyrus, anterior division',\n",
    "       'Cingulate Gyrus, posterior division', 'Precuneous Cortex',\n",
    "       'Cuneal Cortex', 'Frontal Orbital Cortex',\n",
    "       'Parahippocampal Gyrus, anterior division',\n",
    "       'Parahippocampal Gyrus, posterior division', 'Lingual Gyrus',\n",
    "       'Temporal Fusiform Cortex, anterior division',\n",
    "       'Temporal Fusiform Cortex, posterior division',\n",
    "       'Temporal Occipital Fusiform Cortex', 'Occipital Fusiform Gyrus',\n",
    "       'Frontal Opercular Cortex', 'Central Opercular Cortex',\n",
    "       'Parietal Opercular Cortex', 'Planum Polare',\n",
    "       \"Heschl's Gyrus (includes H1 and H2)\", 'Planum Temporale',\n",
    "       'Supracalcarine Cortex', 'Occipital Pole',\n",
    "       'Left Cerebral White Matter', 'Left Cerebral Cortex',\n",
    "       'Left Lateral Ventricle', 'Left Thalamus', 'Left Caudate',\n",
    "       'Left Putamen', 'Left Pallidum', 'Brain-Stem', 'Left Hippocampus',\n",
    "       'Left Amygdala', 'Left Accumbens', 'Right Cerebral White Matter',\n",
    "       'Right Cerebral Cortex', 'Right Lateral Ventricle',\n",
    "       'Right Thalamus', 'Right Caudate', 'Right Putamen',\n",
    "       'Right Pallidum', 'Right Hippocampus', 'Right Amygdala',\n",
    "       'Right Accumbens']\n",
    "\n",
    "I have data with respect to the following tests:\n",
    "\n",
    "{\n",
    "    \"moca_total\": {\n",
    "        \"Description\": \"total score on the Montreal Cognitive Assessment\",\n",
    "        \"Units\": \"based on scoring criteria\"\n",
    "    },\n",
    "    \"drs\": {\n",
    "        \"Description\": \"Mattis Dementia Rating Scale 2\",\n",
    "        \"Units\": \"based on scoring criteria\"\n",
    "    },\n",
    "    \"tmt_a\": {\n",
    "        \"Description\": \"Trail Making Test part A\",\n",
    "        \"Units\": \"seconds\"\n",
    "    },\n",
    "    \"tmt_b\": {\n",
    "        \"Description\": \"Trail Making Test part B\",\n",
    "        \"Units\": \"seconds\"\n",
    "    },\n",
    "    \"bta\": {\n",
    "        \"Description\": \"Brief Test of Attention\",\n",
    "        \"Units\": \"based on scoring criteria\"\n",
    "    },\n",
    "    \"sdmt_oral\": {\n",
    "        \"Description\": \"Symbol Digits Modality Test, oral version\",\n",
    "        \"Units\": \"based on scoring criteria\"\n",
    "    },\n",
    "    \"bnt\": {\n",
    "        \"Description\": \"Boston Naming Test\",\n",
    "        \"Units\": \"based on scoring criteria\"\n",
    "    },\n",
    "    \"cvlt15\": {\n",
    "        \"Description\": \"California Verbal Learning Test, 2nd edition, trials 1-5 total learning score\",\n",
    "        \"Units\": \"based on scoring criteria\"\n",
    "    },\n",
    "    \"cvlt_ld_tot\": {\n",
    "        \"Description\": \"California Verbal Learning Test, 2nd edition, long delay free recall total score\",\n",
    "        \"Units\": \"based on scoring criteria\"\n",
    "    },\n",
    "    \"fas\": {\n",
    "        \"Description\": \"F-A-S Verbal Phonemic Fluency test\",\n",
    "        \"Units\": \"based on scoring criteria\"\n",
    "    },\n",
    "    \"jlo\": {\n",
    "        \"Description\": \"Judgement of Line Orientation test\",\n",
    "        \"Units\": \"based on scoring criteria\"\n",
    "    }\n",
    "}\n",
    "\n",
    "I also have data of the following tests:\n",
    "\n",
    "{\n",
    "    \"attention\": {\n",
    "        \"Description\": \"Attention cognitive composite score\",\n",
    "        \"Units\": \"standardized to norm 0, variance 1\"\n",
    "    },\n",
    "    \"executive\": {\n",
    "        \"Description\": \"Executive function cognitive composite score\",\n",
    "        \"Units\": \"standardized to norm 0, variance 1\"\n",
    "    },\n",
    "    \"global\": {\n",
    "        \"Description\": \"Global cognitive composite score\",\n",
    "        \"Units\": \"standardized to norm 0, variance 1\"\n",
    "    },\n",
    "    \"language\": {\n",
    "        \"Description\": \"Language cognitive composite score\",\n",
    "        \"Units\": \"standardized to norm 0, variance 1\"\n",
    "    },\n",
    "    \"memory\": {\n",
    "        \"Description\": \"Learning and memory cognitive composite score\",\n",
    "        \"Units\": \"standardized to norm 0, variance 1\"\n",
    "    },\n",
    "    \"visuospatial\": {\n",
    "        \"Description\": \"Visuospatial cognitive composite score\",\n",
    "        \"Units\": \"standardized to norm 0, variance 1\"\n",
    "    }\n",
    "}\n",
    "\n",
    "I have attempted to identify and remove correlations in the data, examine only the regions that are associated with fALFF in the DMN (because the patients have Parkinsons MCI and Parkinsons Dementia), fit a KMeans model to the data, examine the first two principal components by looking at the pca_transformation of the kmeans transformed data, but the silhouette score of the kmeans transformed data is lower than I would like (approximately 25%). False Positives are crucial, and given the data and the specifications of the data, there is 100% certainty that the 3 clean clusters exist. How do I identify patients, given this information, that are segregated into groups of healthy cognition, Parkinson's MCI, or Parkinson's Demented?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explaination of the Development of Parkinson's in the Substantia Nigra\n",
    "Refined Explanation of the Process:\n",
    "\n",
    "    Dopamine Metabolism & Hydrogen Peroxide Production:\n",
    "\n",
    "        In substantia nigra (SNc) dopaminergic neurons, dopamine is metabolized primarily by monoamine oxidase (MAO).\n",
    "\n",
    "        MAO converts dopamine into DOPAC (3,4-Dihydroxyphenylacetic acid), producing hydrogen peroxide (H₂O₂) as a byproduct.\n",
    "\n",
    "        Under normal conditions, mitochondria and antioxidant enzymes like glutathione peroxidase neutralize H₂O₂.\n",
    "\n",
    "    Mitochondrial Failure & Oxidative Stress:\n",
    "\n",
    "        In Parkinson’s disease (PD), mitochondrial dysfunction reduces the cell’s ability to process and eliminate reactive oxygen species (ROS) (such as H₂O₂ and superoxide).\n",
    "\n",
    "        Excess ROS damages mitochondrial DNA, lipids, and proteins, worsening cellular stress.\n",
    "\n",
    "    Dopamine Auto-Oxidation & Neurotoxicity:\n",
    "\n",
    "        With mitochondrial failure, excess dopamine that is not properly metabolized undergoes auto-oxidation, producing:\n",
    "\n",
    "            Dopamine-o-quinones (DAQ)\n",
    "\n",
    "            Neuromelanin (a polymerized form of oxidized dopamine byproducts)\n",
    "\n",
    "            Highly reactive ROS (superoxide O₂⁻, hydroxyl radicals ●OH)\n",
    "\n",
    "        These compounds contribute to protein aggregation (including α-synuclein misfolding), oxidative damage, and neuronal toxicity.\n",
    "\n",
    "    Iron Accumulation & Aminochrome Formation:\n",
    "\n",
    "        Iron accumulates abnormally in the substantia nigra of PD patients.\n",
    "\n",
    "        Iron catalyzes dopamine oxidation, forming aminochrome, which is particularly toxic to neurons.\n",
    "\n",
    "        Aminochrome can interact with proteins and lipids, leading to lysosomal and proteasomal dysfunction, which further promotes neurodegeneration.\n",
    "\n",
    "    Brain Acidosis & Cell Death:\n",
    "\n",
    "        Iron-induced dopamine oxidation is accelerated in acidic environments.\n",
    "\n",
    "        PD patients exhibit increased brain acidity (acidosis), which promotes further oxidation and neurotoxic reactions.\n",
    "\n",
    "        The cumulative oxidative stress triggers apoptosis (programmed cell death) via mitochondrial pathways, leading to dopaminergic neuron loss in the substantia nigra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fALFF_df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fALFF_df_scaled_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use K-Means to discover 3 clusters\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "gmm = GaussianMixture(n_components=3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fALFF_df_scaled_imputed.loc[:, DMN].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kmeans.fit(fALFF_df_scaled_imputed.loc[:, DMN])\n",
    "\n",
    "# Assign discovered clusters as target variable\n",
    "# kmeans_y = df[\"Cluster\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gmm_y = gmm.fit_predict(fALFF_df_scaled_imputed.loc[:, DMN])\n",
    "# gmm_y = df[\"Cluster\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = pca.fit_transform(fALFF_df_scaled_imputed.loc[:, DMN])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(perplexity=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_pca_tsne = tsne.fit_transform(X_pca)\n",
    "plt.title(\"KMeans Coloring of Cognition using fALFF\")\n",
    "\n",
    "plt.scatter(X_pca_tsne[:, 0], X_pca_tsne[:, 1], c=kmeans.labels_, cmap=\"viridis\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_pca_tsne[:, 0], X_pca_tsne[:, 1], c=gmm_y, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute hierarchical linkage matrix\n",
    "Z = linkage(fALFF_df_scaled_imputed, method=\"ward\")  # \"ward\" minimizes intra-cluster variance\n",
    "\n",
    "# Assign 3 clusters based on tree cut\n",
    "df[\"Cluster\"] = fcluster(Z, 3, criterion=\"maxclust\")\n",
    "y = df[\"Cluster\"]\n",
    "\n",
    "# Visualize dendrogram\n",
    "sns.clustermap(fALFF_df_scaled_imputed, method=\"ward\", cmap=\"viridis\", figsize=(10, 8))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = pca.fit_transform(cv_df_scaled_imputed.loc[:, DMN])\n",
    "tsne = TSNE(perplexity=5)\n",
    "X_pca_tsne = tsne.fit_transform(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"KMeans Coloring of Cognition using Coefficient of Variation\")\n",
    "\n",
    "plt.scatter(X_pca_tsne[:, 0], X_pca_tsne[:, 1], c=kmeans_cv_y, cmap=\"viridis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance & ANOVA Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_f, anova_p = f_classif(fALFF_df_scaled_imputed.loc[:, DMN], kmeans_y)\n",
    "anova_results = pd.DataFrame({\"Feature\": fALFF_df_scaled_imputed.loc[:, DMN].columns, \"F-Score\": anova_f, \"p-Value\": anova_p})\n",
    "anova_results = anova_results.sort_values(by=\"F-Score\", ascending=False)\n",
    "print(\"ANOVA Results:\\n\", anova_results.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_cv =  KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "kmeans_cv_y = kmeans_cv.fit_predict(cv_df_scaled_imputed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_f, anova_p = f_classif(cv_df_scaled_imputed.loc[:, DMN], kmeans_cv_y)\n",
    "anova_results = pd.DataFrame({\"Feature\": cv_df_scaled_imputed.loc[:, DMN].columns, \"F-Score\": anova_f, \"p-Value\": anova_p})\n",
    "anova_results = anova_results.sort_values(by=\"F-Score\", ascending=False)\n",
    "print(\"ANOVA Results:\\n\", anova_results.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_imputed_df.drop(missing_columns, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
