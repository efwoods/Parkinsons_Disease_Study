{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Proof-of-Concept "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports, Functions, & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import gzip\n",
    "from nilearn import image, masking, plotting, datasets\n",
    "from nilearn.input_data import NiftiMasker\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "##########################################\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn import plotting\n",
    "from nilearn import image\n",
    "from nilearn import regions\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "##########################################\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nilearn.datasets import fetch_atlas_harvard_oxford\n",
    "from scipy.ndimage import zoom\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nilearn.datasets import load_mni152_template\n",
    "from nilearn import plotting\n",
    "from nilearn import datasets\n",
    "from nilearn import plotting\n",
    "\n",
    "\n",
    "from nilearn.plotting import plot_roi\n",
    "\n",
    "from nilearn.image import resample_to_img\n",
    "\n",
    "from scipy.fft import fft, fftfreq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fMRI and anatomical data\n",
    "def load_fmri_data(fmri_filepath):\n",
    "    fmri_img = nib.load(fmri_filepath)\n",
    "    fmri_data = fmri_img.get_fdata()\n",
    "    return fmri_data\n",
    "\n",
    "def load_anatomical_data(anatomical_filepath):\n",
    "    anatomical_img = nib.load(anatomical_filepath)\n",
    "    anatomical_data = anatomical_img.get_fdata()\n",
    "    return anatomical_data\n",
    "\n",
    "def load_nii(file_path):\n",
    "    \"\"\"Load .nii file using nibabel and return data as numpy array.\"\"\"\n",
    "    nii_data = nib.load(file_path)\n",
    "    return nii_data.get_fdata(), nii_data.affine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gz_files = [f for f in glob(f\"./data/**/*.gz\", recursive=True) if os.path.isfile(f)]\n",
    "# gz_files\n",
    "# Extract each .gz file\n",
    "# for gz_file in gz_files:\n",
    "#     output_file = gz_file[:-3]  # Remove \".gz\" extension\n",
    "\n",
    "#     # Decompress the .gz file\n",
    "#     with gzip.open(gz_file, 'rb') as f_in:\n",
    "#         with open(output_file, 'wb') as f_out:\n",
    "#             shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "#     print(f\"Extracted: {gz_file} -> {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Root directory is 'data'\n",
    "root_dir = 'data'\n",
    "\n",
    "# Glob all files that end in .nii under any 'func' directory (in nested subfolders)\n",
    "func_files = glob.glob(os.path.join(root_dir, '**', 'func', '**', '*.nii'), recursive=True)\n",
    "\n",
    "# Filter out files that contain the word 'mask' from the func_files list\n",
    "func_files_without_mask = [file for file in func_files if 'mask' not in file]\n",
    "\n",
    "# Glob all files that end in .nii under any 'anat' directory (in nested subfolders)\n",
    "anat_files = glob.glob(os.path.join(root_dir, '**', 'anat', '**', '*.nii'), recursive=True)\n",
    "\n",
    "# Glob all files that contain the word \"mask\" that end in .nii and are located under any 'func' directory (in nested subfolders)\n",
    "func_mask_files = glob.glob(os.path.join(root_dir, '**', 'func', '**', '*mask*.nii'), recursive=True)\n",
    "\n",
    "# Output the results\n",
    "print(\"Func files without mask:\", func_files_without_mask)\n",
    "print(\"Anat files:\", anat_files)\n",
    "print(\"Func mask files:\", func_mask_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anatomical Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "file_list_len = len(func_files_without_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index += 1\n",
    "index %= file_list_len\n",
    "# Load the compressed NIfTI file\n",
    "img = nib.load(anat_files[index])\n",
    "\n",
    "# Get image data as a NumPy array\n",
    "data = img.get_fdata()\n",
    "\n",
    "# Display a middle slice of the brain\n",
    "slice_index = data.shape[2] // 2  # Middle slice in the z-dimension\n",
    "\n",
    "plt.imshow(data[:, :, slice_index], cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(f\"T1-Weighted MRI Slice: \\n{anat_files[index]}\\n index: {index}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anat_files[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_anat(anat_files[index], title=f\"3D Brain Visualization: \\n{anat_files[index]}\", threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anat_data_f_name = anat_files[index]\n",
    "func_data_f_name = func_files_without_mask[index]\n",
    "\n",
    "# Load the compressed NIfTI file\n",
    "img = nib.load(anat_data_f_name)\n",
    "\n",
    "# Get image data as a NumPy array\n",
    "anat_data = img.get_fdata()\n",
    "\n",
    "# Load the compressed NIfTI file\n",
    "img = nib.load(func_data_f_name)\n",
    "\n",
    "# Get image data as a NumPy array\n",
    "func_data = img.get_fdata()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating A Graph Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Process](https://chatgpt.com/share/67dccc80-8f9c-8011-97b0-f044a525470d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intended Result:\n",
    "# This method allows you to visualize the differences in brain activity between individuals using the networkx graph in a 3D space. By using Three.js, you can provide an interactive and detailed representation of the anatomical brain regions and their connections, colored by activity differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load & Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 4D fMRI data and 3D anatomical data for each brain\n",
    "brains_fmri = func_files_without_mask\n",
    "brains_anatomical = anat_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: Load the first brain's fMRI and anatomical data\n",
    "fmri_data, fmri_affine = load_nii(brains_fmri[0])  # Shape: (x, y, z, t)\n",
    "anatomical_data, anatomical_affine = load_nii(brains_anatomical[0])  # Shape: (x, y, z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract regions of interest (ROIs) from anatomical data\n",
    "# For simplicity, use a predefined atlas (e.g., Harvard-Oxford or AAL)\n",
    "# from nilearn.datasets import load_mni152_template\n",
    "# from nilearn import plotting\n",
    "\n",
    "# Load the MNI template (in standard space) for visualization\n",
    "# mni_template = load_mni152_template()\n",
    "\n",
    "# Load an atlas, for example, Harvard-Oxford sub-cortical regions\n",
    "# from nilearn import datasets\n",
    "# from nilearn import plotting\n",
    "\n",
    "# Load Harvard-Oxford sub-cortical regions (valid name: sub-maxprob-thr50-1mm)\n",
    "atlas = datasets.fetch_atlas_harvard_oxford('sub-maxprob-thr50-1mm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The atlas is returned as a dictionary with paths to the data\n",
    "print(atlas.filename)  # This is the file path to the MNI space atlas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize the Harvard-Oxford sub-cortical regions\n",
    "plotting.plot_roi(atlas.filename, title=\"Harvard-Oxford Sub-Cortical Regions\", display_mode='ortho', draw_cross=True)\n",
    "plotting.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NumPy array (anatomical_data) to a NIfTI image using the affine matrix\n",
    "anat_img = nib.Nifti1Image(anatomical_data, anatomical_affine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the anatomical image\n",
    "plotting.plot_anat(anat_img, title=\"Brain Anatomy\")\n",
    "plotting.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Region Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 4D fMRI data and 3D anatomical data for each brain\n",
    "brains_fmri = func_files_without_mask\n",
    "brains_anatomical = anat_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"brains_fmri: {brains_fmri[0]}\")\n",
    "print(f\"brains_anatomical: {brains_anatomical[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load the first brain's fMRI and anatomical data\n",
    "fmri_data, fmri_affine = load_nii(brains_fmri[0])  # Shape: (x, y, z, t)\n",
    "anatomical_data, anatomical_affine = load_nii(brains_anatomical[0])  # Shape: (x, y, z)\n",
    "anatomical_nii = nib.load(brains_anatomical[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Atlas shape:\", anatomical_data.shape)  # Should be (X, Y, Z)\n",
    "print(\"fMRI shape:\", fmri_data.shape)  # Should be (X, Y, Z, Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the atlas (cortical or subcortical)\n",
    "\n",
    "# atlas = fetch_atlas_harvard_oxford('cort-maxprob-thr25-1mm')  # 1mm resolution\n",
    "# atlas_img = atlas.maps\n",
    "\n",
    "# Fetch the cortical atlas\n",
    "atlas_cortex = fetch_atlas_harvard_oxford('cort-maxprob-thr25-1mm')\n",
    "atlas_img_cortex = atlas_cortex.maps\n",
    "\n",
    "# Resample the atlas to match the anatomical image\n",
    "resampled_atlas = resample_to_img(atlas_img_cortex, anatomical_nii, interpolation='nearest')\n",
    "# resampled_atlas = resample_to_img(atlas_img, anatomical_nii , interpolation='nearest')\n",
    "\n",
    "# Save the new resampled atlas\n",
    "resampled_atlas.to_filename(\"registered_atlas.nii.gz\")\n",
    "plot_roi(resampled_atlas, bg_img=anatomical_nii, title=\"Registered Harvard-Oxford Atlas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the sub-cortical atlas\n",
    "atlas_subcortical = fetch_atlas_harvard_oxford('sub-maxprob-thr25-1mm')\n",
    "atlas_img_subcortical = atlas_subcortical.filename\n",
    "\n",
    "# Resample the atlas to match the anatomical image\n",
    "resampled_atlas = resample_to_img(atlas_img_subcortical, anatomical_nii, interpolation='nearest')\n",
    "# resampled_atlas = resample_to_img(atlas_img, anatomical_nii , interpolation='nearest')\n",
    "\n",
    "# Save the new resampled atlas\n",
    "resampled_atlas.to_filename(\"registered_atlas.nii.gz\")\n",
    "plot_roi(resampled_atlas, bg_img=anatomical_nii, title=\"Registered Harvard-Oxford Atlas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Regions of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the resampled atlas image to a NumPy array\n",
    "atlas_data = resampled_atlas.get_fdata()\n",
    "\n",
    "# Extract unique region labels from the atlas\n",
    "roi_labels = np.unique(atlas_data)[1:]  # Ignore background (0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(atlas_data == 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Resample the atlas to match the spatial dimensions of fmri_data (64, 64, 34)\n",
    "# Determine the scaling factors for each spatial dimension\n",
    "scaling_factors = np.array(fmri_data.shape[:3]) / np.array(atlas_data.shape)\n",
    "\n",
    "# Resample the atlas using the scaling factors (linear interpolation)\n",
    "resampled_atlas = zoom(atlas_data, scaling_factors, order=1)  # Using linear interpolation\n",
    "\n",
    "# Round and cast the resampled atlas to integers\n",
    "resampled_atlas = np.round(resampled_atlas).astype(int)\n",
    "\n",
    "# Step 2: Extract unique region labels from the resampled atlas (skip background 0)\n",
    "roi_labels = np.unique(resampled_atlas)[np.unique(resampled_atlas) > 0]\n",
    "\n",
    "# Create a dictionary to store extracted time series for each ROI\n",
    "roi_time_series = {}\n",
    "\n",
    "# Step 3: Extract fMRI time series for each ROI\n",
    "for roi in tqdm(roi_labels, desc=\"Processing ROIs\"):\n",
    "    # Get the voxel indices for the current ROI in the resampled atlas\n",
    "    roi_voxels = np.where(resampled_atlas == roi)\n",
    "\n",
    "    # Ensure the voxel indices are valid within the spatial dimensions of fmri_data\n",
    "    # Extract the fMRI signal corresponding to these voxels for each time point\n",
    "    roi_signal = fmri_data[roi_voxels].mean(axis=0)  # Averaging across voxels (axis 0 refers to spatial)\n",
    "\n",
    "    # Store the resulting time series for the current ROI\n",
    "    roi_time_series[roi] = roi_signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "roi_df = pd.DataFrame(roi_time_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_df.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSV\n",
    "roi_df.to_csv(\"roi_activity.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an example ROI signal\n",
    "plt.plot(roi_df.iloc[:, 0], label = atlas_subcortical.labels[1])  # Plot the first region\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"BOLD Signal\")\n",
    "plt.title(\"Region Activity Over Time\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an example ROI signal\n",
    "plt.plot(roi_df.iloc[:, index], label = atlas_subcortical.labels[index+1])  # Plot the first region\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"BOLD Signal\")\n",
    "plt.title(f\"{atlas_subcortical.labels[index+1]} Region Activity Over Time\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "index +=1\n",
    "index %= roi_df.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Bipartite Graph Between Brains and Regions with Multiple Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_nodes = [\"Brain_1\", \"Brain_2\"]\n",
    "region_nodes = [\"Hippocampus\", \"Cortex\"]\n",
    "\n",
    "# Create a bipartite graph (simple graph)\n",
    "B = nx.Graph()\n",
    "\n",
    "# Add nodes for brains and regions\n",
    "B.add_nodes_from(brain_nodes, bipartite=0)\n",
    "B.add_nodes_from(region_nodes, bipartite=1)\n",
    "\n",
    "# Add a single edge between \"Brain_1\" and \"Hippocampus\" with multiple attributes\n",
    "B.add_edge(\"Brain_1\", \"Hippocampus\", mean_weight=0.8, variability_weight=0.15)\n",
    "\n",
    "# Print out the edge and its attributes\n",
    "for u, v, data in B.edges(data=True):\n",
    "    print(f\"Edge between {u} and {v}: {data}\")\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Define nodes and the bipartite graph structure\n",
    "brain_nodes = [\"Brain_1\", \"Brain_2\"]\n",
    "region_nodes = [\"Hippocampus\", \"Cortex\"]\n",
    "\n",
    "# Create a bipartite graph (simple graph)\n",
    "B = nx.Graph()\n",
    "\n",
    "# Add nodes for brains and regions\n",
    "B.add_nodes_from(brain_nodes, bipartite=0)\n",
    "B.add_nodes_from(region_nodes, bipartite=1)\n",
    "\n",
    "# Add a single edge between \"Brain_1\" and \"Hippocampus\" with multiple attributes\n",
    "B.add_edge(\"Brain_1\", \"Hippocampus\", mean_weight=0.8, variability_weight=0.15)\n",
    "\n",
    "# Set up position for bipartite layout\n",
    "pos = {}\n",
    "pos.update((node, (1, index)) for index, node in enumerate(brain_nodes))  # Position for brain nodes\n",
    "pos.update((node, (2, index)) for index, node in enumerate(region_nodes))  # Position for region nodes\n",
    "\n",
    "# Draw the graph with labels and attributes\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Draw the nodes and edges\n",
    "nx.draw(B, pos, with_labels=True, node_color='skyblue', node_size=2000, font_size=12, font_weight='bold', edge_color='gray')\n",
    "\n",
    "# Display edge labels (mean_weight, variability_weight)\n",
    "edge_labels = {(u, v): f\"Mean: {data['mean_weight']}, Variability: {data['variability_weight']}\" \n",
    "               for u, v, data in B.edges(data=True)}\n",
    "nx.draw_networkx_edge_labels(B, pos, edge_labels=edge_labels, font_size=10)\n",
    "\n",
    "# Show the plot\n",
    "plt.title(\"Bipartite Graph of Brain Regions and Nodes with Attributes\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the ALFF/fALFF and the Bold Variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_df.iloc[:, index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "TR = 2 # Repetition time (in seconds)\n",
    "low_freq_band = (0.01, 0.1) # Low-frequency range for ALFF (Hz)\n",
    "n_timepoints = roi_df.shape[0]\n",
    "frequencies = fftfreq(n_timepoints, d=TR)\n",
    "fft_data = fft(roi_df.iloc[:, index], axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_df.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a mask for the low-frequency band\n",
    "low_freq_mask = np.logical_and(frequencies >= low_freq_band[0], frequencies <= low_freq_band[1])\n",
    "\n",
    "# Compute the ALFF: average amplitude in the low-frequency band\n",
    "alff = np.abs(fft_data)[..., low_freq_mask].mean(axis=-1)\n",
    "\n",
    "# For fALFF: Compute the total power in the 0.01-0.25 Hz range\n",
    "total_power = np.abs(fft_data)[..., (frequencies >= 0.01) & (frequencies <= 0.25)].sum(axis=-1)\n",
    "\n",
    "# Compute fALFF: Normalized ALFF\n",
    "falff = alff / total_power\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOLD Signal Variability (e.g., using Coefficient of Variation)\n",
    "cv = np.std(roi_df.iloc[:, index]) / np.mean(roi_df.iloc[:, index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- [ ] Creating region_activities all_brains:  a list of dictionaries, where each dictionary maps region names to activity levels\n",
    "- [ ] regions: atlas.labels[1:]\n",
    "- [ ] brains_fmri: exists\n",
    "- [ ] Improvement: increase granularity by having multiple signals from regions where regions correspond to Link-sized regions with 1024 BOLD temporal signals\n",
    "- [ ] Compute the Activity Difference\n",
    "- [ ] Create the Graph\n",
    "- [ ] Export the graph data\n",
    "- [ ] visualize\n",
    "- [ ] host visualization on github pages\n",
    "- [ ] include sub-cortical regions\n",
    "- [ ] cluster the data by age, handedness, sex, and education\n",
    "- [ ] cluster by health, MCI, and Demented\n",
    "- [ ] evaluate level of stimulation to apply to achieve normal cognitive status\n",
    "- [ ] simulate applying stimulation to MCI or Demented to achieve Healthy Cognitive Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquiring All Signals For all Brains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the atlas (cortical or subcortical)\n",
    "atlas_cortex = fetch_atlas_harvard_oxford('cort-maxprob-thr25-1mm')  # 1mm resolution\n",
    "atlas_img_cortex = atlas.filename\n",
    "\n",
    "# Fetch the sub-cortical atlas\n",
    "atlas_subcortical = fetch_atlas_harvard_oxford('sub-maxprob-thr25-1mm')\n",
    "atlas_img_subcortical = atlas_subcortical.filename\n",
    "\n",
    "atlas_labels = atlas_cortex.labels[1:] + atlas_subcortical.labels[1:]\n",
    "\n",
    "# Root directory is 'data'\n",
    "root_dir = '../data'\n",
    "\n",
    "# Glob all files that end in .nii under any 'func' directory (in nested subfolders)\n",
    "func_files = glob.glob(os.path.join(root_dir, '**', 'func', '**', '*.nii.gz'), recursive=True)\n",
    "\n",
    "# Filter out files that contain the word 'mask' from the func_files list\n",
    "func_files_without_mask = [file for file in func_files if 'mask' not in file]\n",
    "\n",
    "# Glob all files that end in .nii under any 'anat' directory (in nested subfolders)\n",
    "anat_files = glob.glob(os.path.join(root_dir, '**', 'anat', '**', '*.nii.gz'), recursive=True)\n",
    "\n",
    "# Glob all files that contain the word \"mask\" that end in .nii and are located under any 'func' directory (in nested subfolders)\n",
    "func_mask_files = glob.glob(os.path.join(root_dir, '**', 'func', '**', '*mask*.nii.gz'), recursive=True)\n",
    "\n",
    "# Output the results\n",
    "print(\"Func files without mask:\", func_files_without_mask)\n",
    "print(\"Anat files:\", anat_files)\n",
    "print(\"Func mask files:\", func_mask_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 4D fMRI data and 3D anatomical data for each brain\n",
    "brains_fmri = func_files_without_mask\n",
    "brains_anatomical = anat_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(brains_anatomical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(brains_fmri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bold Signals For All Subcortical and Cortical Regions\n",
    "for person in tqdm(range(len(brains_fmri))):\n",
    "    bold_signal_dir = os.path.dirname(os.path.dirname(brains_fmri[person]))\n",
    "    subject_name = os.path.basename(os.path.dirname(os.path.dirname(os.path.dirname(brains_fmri[person]))))\n",
    "    # Example: Load the first brain's fMRI and anatomical data\n",
    "    fmri_data, fmri_affine = load_nii(brains_fmri[person])  # Shape: (x, y, z, t)\n",
    "    anatomical_data, anatomical_affine = load_nii(brains_anatomical[person])  # Shape: (x, y, z)\n",
    "    anatomical_nii = nib.load(brains_anatomical[person])\n",
    "    \n",
    "    # Resample the atlas to match the anatomical image (cortical)\n",
    "    resampled_atlas_cortex = resample_to_img(atlas_img_cortex, anatomical_nii , interpolation='nearest')\n",
    "    resampled_atlas_subcortex = resample_to_img(atlas_img_subcortical, anatomical_nii , interpolation='nearest')\n",
    "    \n",
    "    # Save the new resampled atlas\n",
    "    # resampled_atlas.to_filename(\"registered_atlas.nii.gz\")\n",
    "    \n",
    "    # Convert the resampled atlas image to a NumPy array\n",
    "    atlas_data_cortex = resampled_atlas_cortex.get_fdata()\n",
    "    atlas_data_subcortex = resampled_atlas_subcortex.get_fdata()\n",
    "    \n",
    "    # Extract unique region labels from the atlas\n",
    "    roi_labels_cortex = np.unique(atlas_data_cortex)[1:]  # Ignore background (0)\n",
    "    roi_labels_subcortex = np.unique(atlas_data_subcortex)[1:]  # Ignore background (0)roi_labels_cortex = np.unique(atlas_data_cortex)[1:]  # Ignore background (0)\n",
    "\n",
    "    # Step 1: Resample the atlas to match the spatial dimensions of fmri_data (64, 64, 34)\n",
    "    # Determine the scaling factors for each spatial dimension\n",
    "    scaling_factors = np.array(fmri_data.shape[:3]) / np.array(atlas_data_cortex.shape)\n",
    "\n",
    "    # Resample the atlas using the scaling factors (linear interpolation)\n",
    "    resampled_atlas_cortex = zoom(atlas_data_cortex, scaling_factors, order=1)  # Using linear interpolation\n",
    "    resampled_atlas_subcortex = zoom(atlas_data_subcortex, scaling_factors, order=1) # Using linear inerpolation\n",
    "\n",
    "    # Round and cast the resampled atlas to integers\n",
    "    resampled_atlas_cortex = np.round(resampled_atlas_cortex).astype(int)\n",
    "    resampled_atlas_subcortex = np.round(resampled_atlas_subcortex).astype(int)\n",
    "\n",
    "    # Step 2: Extract unique region labels from the resampled atlas (skip background 0)\n",
    "    # roi_labels = np.unique(resampled_atlas)[np.unique(resampled_atlas) > 0]\n",
    "\n",
    "    # Create a dictionary to store extracted time series for each ROI\n",
    "    roi_time_series = {}\n",
    "\n",
    "    # Step 3: Extract fMRI time series for each ROI\n",
    "    for roi in tqdm(roi_labels_cortex, desc=\"Processing ROIs\"):\n",
    "        # Get the voxel indices for the current ROI in the resampled atlas\n",
    "        roi_voxels = np.where(resampled_atlas_cortex == roi)\n",
    "\n",
    "        # Ensure the voxel indices are valid within the spatial dimensions of fmri_data\n",
    "        # Extract the fMRI signal corresponding to these voxels for each time point\n",
    "        roi_signal = fmri_data[roi_voxels].mean(axis=0)  # Averaging across voxels (axis 0 refers to spatial)\n",
    "\n",
    "        # Store the resulting time series for the current ROI\n",
    "        roi_time_series[roi] = roi_signal\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    roi_cortex_df = pd.DataFrame(roi_time_series)\n",
    "\n",
    "    # Create a dictionary to store extracted time series for each ROI\n",
    "    roi_time_series = {}\n",
    "\n",
    "    # Step 3: Extract fMRI time series for each ROI\n",
    "    for roi in tqdm(roi_labels_subcortex, desc=\"Processing ROIs\"):\n",
    "        # Get the voxel indices for the current ROI in the resampled atlas\n",
    "        roi_voxels = np.where(resampled_atlas_subcortex == roi)\n",
    "\n",
    "        # Ensure the voxel indices are valid within the spatial dimensions of fmri_data\n",
    "        # Extract the fMRI signal corresponding to these voxels for each time point\n",
    "        roi_signal = fmri_data[roi_voxels].mean(axis=0)  # Averaging across voxels (axis 0 refers to spatial)\n",
    "\n",
    "        # Store the resulting time series for the current ROI\n",
    "        roi_time_series[roi] = roi_signal\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    roi_subcortex_df = pd.DataFrame(roi_time_series)\n",
    "    roi_df = pd.concat([roi_cortex_df, roi_subcortex_df], axis=1)\n",
    "    roi_df.columns = atlas_labels\n",
    "\n",
    "    f_path = bold_signal_dir + '/' + subject_name + \"_roi_BOLD_activity.csv\"\n",
    "\n",
    "    # Save as CSV\n",
    "    bold_signal_dir\n",
    "    # roi_df.to_csv(f_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting CSV of BOLD Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 4D fMRI data and 3D anatomical data for each brain\n",
    "brains_fmri = func_files_without_mask\n",
    "brains_anatomical = anat_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOLD_activity_csv_list =[f for f in glob.glob(\"data/**/*.csv\", recursive=True) if \"dataframes\" not in f ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOLD_activity_csv_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the atlas (cortical or subcortical)\n",
    "atlas_cortex = fetch_atlas_harvard_oxford('cort-maxprob-thr25-1mm')  # 1mm resolution\n",
    "atlas_img_cortex = atlas_cortex.filename\n",
    "\n",
    "# Fetch the sub-cortical atlas\n",
    "atlas_subcortical = fetch_atlas_harvard_oxford('sub-maxprob-thr25-1mm')\n",
    "atlas_img_subcortical = atlas_subcortical.filename\n",
    "\n",
    "atlas_labels = atlas_cortex.labels[1:] + atlas_subcortical.labels[1:]\n",
    "atlas_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOLD_activity_df_list = []\n",
    "for person in BOLD_activity_csv_list:\n",
    "    BOLD_activity_df_list.append(pd.read_csv(person))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computing fractional Amplitude Low Frequency Fluctuations & Coefficient of Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fALFF_df = pd.DataFrame(columns=[\"Subject\"] + atlas_labels)\n",
    "cv_df = pd.DataFrame(columns=[\"Subject\"] + atlas_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "TR = 2 # Repetition time (in seconds)\n",
    "low_freq_band = (0.01, 0.1) # Low-frequency range for ALFF (Hz)\n",
    "n_timepoints = 300\n",
    "frequencies = fftfreq(n_timepoints, d=TR)\n",
    "# Create a mask for the low-frequency band\n",
    "low_freq_mask = np.logical_and(frequencies >= low_freq_band[0], frequencies <= low_freq_band[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fft(BOLD_activity_df_list[0].iloc[:, 1], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp_df in BOLD_activity_df_list:\n",
    "    for region in range(len(temp_df.columns)):\n",
    "        # print(temp_df.iloc[:, region])\n",
    "        print(temp_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.clumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.iloc[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp_df in BOLD_activity_df_list:\n",
    "    for region in range(len(temp_df.columns)):\n",
    "        print(temp_df.iloc[:, region])\n",
    "        print(type(region))\n",
    "        # fft_data = fft(temp_df.iloc[:, region], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Frequencies and CV\n",
    "person = 0\n",
    "for temp_df in tqdm(BOLD_activity_df_list):\n",
    "    subject_name = os.path.basename(os.path.dirname(os.path.dirname(os.path.dirname(brains_fmri[person]))))\n",
    "    fALFF_values = []\n",
    "    cv_values = []\n",
    "    for region in range(len(temp_df.columns)):\n",
    "        fft_data = fft(temp_df.iloc[:, region], axis=-1)\n",
    "        \n",
    "        # Compute the ALFF: average amplitude in the low-frequency band\n",
    "        alff = np.abs(fft_data)[..., low_freq_mask].mean(axis=-1)\n",
    "\n",
    "        # For fALFF: Compute the total power in the 0.01-0.25 Hz range\n",
    "        total_power = np.abs(fft_data)[..., (frequencies >= 0.01) & (frequencies <= 0.25)].sum(axis=-1)\n",
    "\n",
    "        # Compute fALFF: Normalized ALFF\n",
    "        falff = alff / total_power\n",
    "        fALFF_values.append(falff)\n",
    "        \n",
    "        # BOLD Signal Variability (e.g., using Coefficient of Variation)\n",
    "        cv = np.std(temp_df.iloc[:, region]) / np.mean(temp_df.iloc[:, region])\n",
    "        cv_values.append(cv)\n",
    "    \n",
    "    # Create the new row with \"Subject\" and the numerical values\n",
    "    new_row = {'Subject': subject_name}  # Add the string value for 'Subject'\n",
    "    new_row.update(dict(zip(atlas_labels, fALFF_values)))  # Add numerical values to the other columns\n",
    "    fALFF_df.loc[len(fALFF_df)] = new_row\n",
    "    \n",
    "    new_row = {'Subject': subject_name}\n",
    "    new_row.update(dict(zip(atlas_labels, cv_values)))\n",
    "    cv_df.loc[len(cv_df)] = new_row\n",
    "    person+=1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(BOLD_activity_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subject_name_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dir = \"data/dataframes/BOLD/\"\n",
    "for df in tqdm(range(len(BOLD_activity_df_list))):\n",
    "    BOLD_activity_df_list[df].to_csv(df_dir + subject_name_l[df] + \"_BOLD.csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fALFF_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the 'Subject' column as the index for easy access\n",
    "fALFF_df.set_index('Subject', inplace=True)\n",
    "cv_df.set_index('Subject', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save fALFF_df & cv df\n",
    "fALFF_df.to_csv(\"data/dataframes/fALFF.csv\")  # Save without the index\n",
    "cv_df.to_csv(\"data/dataframes/cv_df.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acquiring subject names:\n",
    "subject_name_l = []\n",
    "for person in range(len(brains_fmri)):\n",
    "    subject_name = os.path.basename(os.path.dirname(os.path.dirname(os.path.dirname(brains_fmri[person]))))\n",
    "    subject_name_l.append(subject_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_name_df = pd.DataFrame(subject_name_l, columns=['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_name_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_name_df.to_csv(\"data/dataframes/subject_name_df.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the fALFF & CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fALFF & CV\n",
    "fALFF_df = pd.read_csv(\"data/dataframes/fALFF.csv\", index_col=0)\n",
    "cv_df = pd.read_csv(\"data/dataframes/cv_df.csv\", index_col=0)\n",
    "subject_name_df = pd.read_csv(\"data/dataframes/subject_name_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_name_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fALFF_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOLD_activity_df_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty bipartite graph\n",
    "B = nx.Graph()\n",
    "\n",
    "# Add nodes for subjects (bipartite set 1)\n",
    "B.add_nodes_from(fALFF_df.index, bipartite=0)\n",
    "\n",
    "# Add nodes for regions (bipartite set 2)\n",
    "regions = fALFF_df.columns.tolist()  # All regions are the columns of fALFF_df\n",
    "B.add_nodes_from(regions, bipartite=1)\n",
    "\n",
    "# Add edges between subjects and regions with two distinct weights (one from fALFF_df and one from cv_df)\n",
    "for subject in fALFF_df.index:\n",
    "    for region in fALFF_df.columns:\n",
    "        weight1 = fALFF_df.at[subject, region]  # Weight from fALFF_df\n",
    "        weight2 = cv_df.at[subject, region]  # Weight from cv_df\n",
    "        \n",
    "        # Add an edge with two distinct weights\n",
    "        B.add_edge(subject, region, weight_fALFF=weight1, weight_cv=weight2)\n",
    "\n",
    "# Now the graph B contains bipartite nodes (subjects and regions) with edges having two distinct weights\n",
    "\n",
    "# Example: Print the edges with weights from both dataframes\n",
    "for u, v, data in B.edges(data=True):\n",
    "    print(f\"Edge ({u}, {v}) - Weight from fALFF_df: {data['weight_fALFF']}, Weight from cv_df: {data['weight_cv']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Set positions for bipartite layout with increased spacing\n",
    "pos = {}\n",
    "pos.update((node, (1, index * 2.5)) for index, node in enumerate(fALFF_df.index))  # Subjects (left side)\n",
    "pos.update((node, (4, index * 2.5)) for index, node in enumerate(regions))  # Regions (right side)\n",
    "\n",
    "# Get edge weights from the fALFF_df's weight column\n",
    "edge_weights = [B[u][v]['weight_fALFF'] for u, v in B.edges()]\n",
    "\n",
    "# Normalize edge thickness for better visibility\n",
    "max_weight = max(edge_weights) if edge_weights else 1  # Avoid division by zero\n",
    "edge_widths = [1 + 6 * (w / max_weight) for w in edge_weights]  # Scale thickness between 1 and 7\n",
    "\n",
    "# Normalize edge color mapping\n",
    "norm = mcolors.Normalize(vmin=min(edge_weights), vmax=max(edge_weights))\n",
    "cmap = cm.Blues\n",
    "edge_colors = [cmap(norm(w)) for w in edge_weights]\n",
    "\n",
    "# Set larger figure size\n",
    "plt.figure(figsize=(24, 16))  # Bigger canvas for better visibility\n",
    "\n",
    "# Draw the bipartite graph with smaller nodes\n",
    "edges = nx.draw_networkx_edges(\n",
    "    B, pos, edge_color=edge_colors, width=edge_widths, edge_cmap=cmap, alpha=0.8\n",
    ")\n",
    "nodes = nx.draw_networkx_nodes(B, pos, node_size=800, node_color='skyblue')  # Smaller nodes\n",
    "labels = nx.draw_networkx_labels(B, pos, font_size=12, font_weight='bold')\n",
    "\n",
    "# Add colorbar legend for edge weights\n",
    "sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, ax=plt.gca(), orientation='vertical', fraction=0.02, pad=0.02)\n",
    "cbar.set_label(\"Edge Weight (fALFF)\", fontsize=14)\n",
    "\n",
    "# Title\n",
    "plt.title(\"Optimized Bipartite Graph with Improved Spacing and Smaller Nodes\", fontsize=16)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the region to visualize\n",
    "region_to_visualize = \"Frontal Pole\"\n",
    "\n",
    "# Create an empty graph\n",
    "B = nx.Graph()\n",
    "\n",
    "# Add the central brain region node\n",
    "B.add_node(region_to_visualize, color='Teal', size=3000)\n",
    "\n",
    "# Add subject nodes and edges\n",
    "for subject in fALFF_df.index:\n",
    "    weight1 = fALFF_df.at[subject, region_to_visualize]  # fALFF weight\n",
    "    weight2 = cv_df.at[subject, region_to_visualize] if region_to_visualize in cv_df.columns else 0  # CV weight\n",
    "    \n",
    "    B.add_node(subject, color='green', size=1500)  # Subject nodes\n",
    "    B.add_edge(subject, region_to_visualize, weight1=weight1, weight2=weight2)  # Edge with weights\n",
    "\n",
    "# Set figure size **before drawing**\n",
    "plt.figure(figsize=(18, 14))  # Larger figure size for better visibility\n",
    "\n",
    "# Positioning: Use `spring_layout` with a **higher `k` value** to spread nodes more\n",
    "pos = nx.spring_layout(B, seed=42, k=1.5)  # Increase k for wider spacing\n",
    "\n",
    "# Draw nodes with custom sizes and colors\n",
    "node_colors = [B.nodes[node]['color'] for node in B.nodes()]\n",
    "node_sizes = [B.nodes[node]['size'] for node in B.nodes()]\n",
    "nx.draw(B, pos, with_labels=True, node_size=node_sizes, node_color=node_colors, font_size=12, font_weight='bold')\n",
    "\n",
    "# Draw edges with labels (4 decimal places)\n",
    "edge_labels = {\n",
    "    (u, v): f\"fALFF: {B[u][v]['weight1']:.4f}\\nCV: {B[u][v]['weight2']:.4f}\" \n",
    "    for u, v in B.edges()\n",
    "}\n",
    "nx.draw_networkx_edge_labels(B, pos, edge_labels=edge_labels, font_size=10)\n",
    "\n",
    "# Title\n",
    "plt.title(f\"Graph Representation: '{region_to_visualize}' Connected to Subjects\", fontsize=14)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Activity Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Assuming `region_activities_all_brains` is a list of dictionaries, where each dictionary maps region names to activity levels\n",
    "# regions = List of brain regions\n",
    "# brains_fmri = List of brain names or identifiers\n",
    "\n",
    "# Loop through each pair of brains (individuals)\n",
    "for i, brain_1 in enumerate(brains_fmri):\n",
    "    for j, brain_2 in enumerate(brains_fmri):\n",
    "        if i < j:  # Make sure you don't compare the same brain to itself\n",
    "            # Loop through each region to calculate the difference for each region separately\n",
    "            for region in regions:\n",
    "                # Calculate the absolute difference in activity for this region between the two brains\n",
    "                activity_diff = np.abs(region_activities_all_brains[i][region] - region_activities_all_brains[j][region])\n",
    "                \n",
    "                # Add an edge between the two brains for this region, with weight as the activity difference for this region\n",
    "                G.add_edge(f\"{brain_1}_{region}\", f\"{brain_2}_{region}\", weight=activity_diff)\n",
    "\n",
    "# Now `G` contains edges where each edge represents the activity difference for a specific region between two brains.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Points:\n",
    "\n",
    "    Graph Structure: Each pair of brains (brain_1 and brain_2) has a separate edge for each region in the brain, with the edge weight representing the difference in activity for that specific region.\n",
    "    Edges Represent Region-Specific Differences: The graph G will have edges named like brain_1_region_name to brain_2_region_name, where region_name corresponds to a specific brain region (e.g., \"hippocampus\", \"prefrontal_cortex\", etc.). The weight of the edge will represent the difference in activity for that region.\n",
    "\n",
    "Example of the Graph Structure:\n",
    "\n",
    "    If there are three brains (brain_1, brain_2, brain_3) and three regions (region_1, region_2, region_3), the graph will have edges like:\n",
    "        brain_1_region_1 to brain_2_region_1 (weight = difference in activity for region_1 between brain_1 and brain_2)\n",
    "        brain_1_region_2 to brain_2_region_2 (weight = difference in activity for region_2 between brain_1 and brain_2)\n",
    "        etc.\n",
    "\n",
    "Usage:\n",
    "\n",
    "    You can then visualize the graph, where each region will have its own node connecting the brains.\n",
    "    You can also use this graph to explore how the activity differences between individual brains vary across regions, which could give insights into specific regional brain activity changes between individuals.\n",
    "\n",
    "If you'd like to visualize or analyze the resulting graph, you can plot the graph using matplotlib, networkx's built-in plotting functions, or any other visualization library you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming `G` is your networkx graph (created in the previous step)\n",
    "\n",
    "# Set up the figure\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# Draw the networkx graph\n",
    "pos = nx.spring_layout(G, k=0.15, iterations=20)  # Layout for positioning nodes\n",
    "nx.draw(G, pos, with_labels=True, node_size=500, node_color=\"lightblue\", font_size=10, font_weight=\"bold\", edge_color='gray')\n",
    "\n",
    "# Add edge labels (weights for the activity differences)\n",
    "edge_labels = nx.get_edge_attributes(G, 'weight')\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "\n",
    "# Display the plot\n",
    "plt.title(\"Brain Activity Differences Between Individuals\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "    spring_layout: This layout is good for general graph visualization. It arranges nodes in 2D space with a force-directed algorithm.\n",
    "    nx.draw(): Draws the nodes and edges.\n",
    "    nx.draw_networkx_edge_labels(): Displays the edge labels, which in this case are the activity differences between regions for each brain pair.\n",
    "\n",
    "This Python code will provide a basic 2D visualization. However, for a 3D interactive plot, you need to export the data and use it with Three.js.\n",
    "Step 2: 3D Visualization with Three.js\n",
    "\n",
    "To visualize the same graph in 3D using Three.js, we need to:\n",
    "\n",
    "    Export the graph data (nodes and edges with weights) into a format that Three.js can use (like JSON).\n",
    "    Create the Three.js scene to visualize the nodes and edges.\n",
    "\n",
    "2.1: Exporting the Graph Data to JSON (Python)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Prepare graph data in a format suitable for Three.js\n",
    "graph_data = {\n",
    "    \"nodes\": [],\n",
    "    \"edges\": []\n",
    "}\n",
    "\n",
    "# Add nodes (brains and regions)\n",
    "for node in G.nodes:\n",
    "    # Extract brain and region from node (e.g., brain_1_region_1)\n",
    "    brain, region = node.split('_')\n",
    "    graph_data[\"nodes\"].append({\n",
    "        \"id\": node,\n",
    "        \"brain\": brain,\n",
    "        \"region\": region\n",
    "    })\n",
    "\n",
    "# Add edges (activity differences between brain pairs)\n",
    "for u, v, data in G.edges(data=True):\n",
    "    graph_data[\"edges\"].append({\n",
    "        \"source\": u,\n",
    "        \"target\": v,\n",
    "        \"weight\": data[\"weight\"]\n",
    "    })\n",
    "\n",
    "# Save to a JSON file\n",
    "with open(\"graph_data.json\", \"w\") as f:\n",
    "    json.dump(graph_data, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will generate a graph_data.json file that contains the nodes (with regions and brain identifiers) and edges (with weights representing the activity differences).\n",
    "2.2: Setting Up Three.js for 3D Visualization\n",
    "\n",
    "Now, lets create the 3D visualization in Three.js. Below is an example of how you can visualize the nodes and edges of your graph in 3D.\n",
    "\n",
    "Here is the basic Three.js setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Brain Activity Differences Visualization</title>\n",
    "    <style>\n",
    "        body { margin: 0; overflow: hidden; }\n",
    "        canvas { display: block; }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js\"></script>\n",
    "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/OrbitControls.min.js\"></script>\n",
    "\n",
    "<script>\n",
    "// Set up the 3D scene, camera, and renderer\n",
    "const scene = new THREE.Scene();\n",
    "const camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);\n",
    "const renderer = new THREE.WebGLRenderer();\n",
    "renderer.setSize(window.innerWidth, window.innerHeight);\n",
    "document.body.appendChild(renderer.domElement);\n",
    "\n",
    "// Load graph data from the JSON file\n",
    "fetch('graph_data.json')\n",
    "    .then(response => response.json())\n",
    "    .then(graphData => {\n",
    "        const nodes = graphData.nodes;\n",
    "        const edges = graphData.edges;\n",
    "\n",
    "        // Add nodes to the scene (each node is a sphere)\n",
    "        const nodeRadius = 0.5;\n",
    "        const nodeGeometry = new THREE.SphereGeometry(nodeRadius, 16, 16);\n",
    "        const nodeMaterial = new THREE.MeshBasicMaterial({ color: 0x00ff00 });\n",
    "        const nodeMeshes = {};\n",
    "\n",
    "        nodes.forEach(node => {\n",
    "            const nodeMesh = new THREE.Mesh(nodeGeometry, nodeMaterial);\n",
    "            // Random positions for nodes, but can be improved later with better layout algorithms\n",
    "            nodeMesh.position.set(Math.random() * 10 - 5, Math.random() * 10 - 5, Math.random() * 10 - 5);\n",
    "            scene.add(nodeMesh);\n",
    "            nodeMeshes[node.id] = nodeMesh;\n",
    "        });\n",
    "\n",
    "        // Add edges between nodes (lines)\n",
    "        const edgeMaterial = new THREE.LineBasicMaterial({ color: 0x0000ff });\n",
    "        edges.forEach(edge => {\n",
    "            const source = nodeMeshes[edge.source];\n",
    "            const target = nodeMeshes[edge.target];\n",
    "            if (source && target) {\n",
    "                const points = [];\n",
    "                points.push(source.position);\n",
    "                points.push(target.position);\n",
    "                const lineGeometry = new THREE.BufferGeometry().setFromPoints(points);\n",
    "                const line = new THREE.Line(lineGeometry, edgeMaterial);\n",
    "                scene.add(line);\n",
    "            }\n",
    "        });\n",
    "\n",
    "        // Set up the camera position\n",
    "        camera.position.z = 20;\n",
    "\n",
    "        // Add orbit controls to move around the scene\n",
    "        const controls = new THREE.OrbitControls(camera, renderer.domElement);\n",
    "\n",
    "        // Animation loop\n",
    "        function animate() {\n",
    "            requestAnimationFrame(animate);\n",
    "            \n",
    "            // Update the controls to enable interaction\n",
    "            controls.update(); // only required if controls.enableDamping or controls.auto-rotation is enabled\n",
    "            \n",
    "            renderer.render(scene, camera);\n",
    "        }\n",
    "        animate();\n",
    "    });\n",
    "\n",
    "</script>\n",
    "\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "source": [
    "Explanation:\n",
    "\n",
    "    Three.js Setup: The script sets up a basic 3D scene with a camera, lighting, and controls for rotating the view.\n",
    "    Graph Data: The graph data (graph_data.json) is loaded into the scene. Nodes are visualized as spheres, and edges are represented as lines connecting the nodes.\n",
    "    Node and Edge Visualization: The nodes are positioned randomly for simplicity, but you can improve the layout with algorithms such as force-directed layout. The edges are created by connecting pairs of nodes.\n",
    "\n",
    "How to Host on GitHub:\n",
    "\n",
    "    Push the graph_data.json file and your Three.js HTML file to your GitHub repository.\n",
    "    Use GitHub Pages to serve your index.html file. You can enable this in the repository settings.\n",
    "\n",
    "Final Thoughts:\n",
    "\n",
    "    3D Visualization: The above Three.js code gives a basic 3D interactive view of the graph. You can further refine the visualization by adjusting the node placement, adding better color schemes, and improving the camera interaction.\n",
    "    Interactivity: With OrbitControls, you can rotate, zoom, and pan the scene, making it easier to inspect the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "source": [
    "### Host the 3D visualization on GitHub Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "source": [
    "To host your 3D visualization on GitHub, you can follow these steps:\n",
    "Step 1: Create a GitHub Repository\n",
    "\n",
    "    Go to GitHub: Navigate to GitHub.\n",
    "    Create a New Repository:\n",
    "        Click the + icon in the top right corner and select \"New repository.\"\n",
    "        Name your repository, for example, brain-visualization.\n",
    "        Optionally, add a description.\n",
    "        Set the repository to \"Public.\"\n",
    "        Initialize with a README file.\n",
    "        Click \"Create repository.\"\n",
    "\n",
    "Step 2: Upload Your Files to GitHub\n",
    "\n",
    "    Clone Your Repository:\n",
    "        Copy the repository URL (e.g., https://github.com/your-username/brain-visualization.git).\n",
    "        Open a terminal and clone the repository to your local machine:\n",
    "\n",
    "    git clone https://github.com/your-username/brain-visualization.git\n",
    "    cd brain-visualization\n",
    "\n",
    "Add Your Files:\n",
    "\n",
    "    Place your HTML file (index.html) and any required assets (such as 3D brain model files, JavaScript files, etc.) inside this repository folder on your local machine.\n",
    "\n",
    "Add and Commit the Changes:\n",
    "\n",
    "    In your terminal, add all files and commit them:\n",
    "\n",
    "    git add .\n",
    "    git commit -m \"Initial commit with 3D brain visualization\"\n",
    "\n",
    "Push the Changes:\n",
    "\n",
    "    Push the changes to GitHub:\n",
    "\n",
    "        git push origin main\n",
    "\n",
    "Step 3: Host the Site Using GitHub Pages\n",
    "\n",
    "    Navigate to the Repository Settings:\n",
    "        Go to your GitHub repository.\n",
    "        Click on the Settings tab at the top of the page.\n",
    "\n",
    "    Enable GitHub Pages:\n",
    "        Scroll down to the GitHub Pages section.\n",
    "        In the Source dropdown, select the main branch.\n",
    "        Click Save.\n",
    "\n",
    "    Access the Hosted Site:\n",
    "        After a few moments, your site will be live at https://your-username.github.io/brain-visualization/.\n",
    "\n",
    "Step 4: Update the Files (If Needed)\n",
    "\n",
    "    Any changes you make locally can be updated to GitHub Pages by following these steps:\n",
    "        Edit your files locally.\n",
    "        Add, commit, and push the changes:\n",
    "\n",
    "        git add .\n",
    "        git commit -m \"Updated the 3D brain visualization\"\n",
    "        git push origin main\n",
    "\n",
    "Your changes will be reflected on the live site after a few moments.\n",
    "Step 5: Access the Model and Scripts\n",
    "\n",
    "Make sure that any assets you reference in your code (e.g., the 3D brain model .obj, JS files, etc.) are either:\n",
    "\n",
    "    Uploaded to the repository and referenced with relative paths, or\n",
    "    Hosted externally (e.g., from a CDN) and referenced via URLs.\n",
    "\n",
    "For example:\n",
    "\n",
    "    If you uploaded the brain model as brain_model.obj to the repository, reference it like this in your index.html:\n",
    "\n",
    "    loader.load('brain_model.obj', function (object) {\n",
    "        scene.add(object);\n",
    "        object.scale.set(2, 2, 2);  // Adjust size\n",
    "    });\n",
    "\n",
    "If your brain model is hosted on a CDN or another server, use its full URL in place of 'brain_model.obj'.\n",
    "Step 6: Debug and Test\n",
    "\n",
    "    After hosting the site, test it by navigating to the GitHub Pages URL in your browser.\n",
    "    If you encounter issues with file paths or assets, ensure the file paths are correct and relative to the GitHub Pages root directory.\n",
    "\n",
    "Example File Structure:\n",
    "\n",
    "brain-visualization/\n",
    " index.html\n",
    " brain_model.obj\n",
    " script.js\n",
    " styles.css\n",
    "\n",
    "By following these steps, you'll have your 3D brain visualization up and running on GitHub Pages!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "source": [
    "### Creating a Brain Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "source": [
    "1. Python Code for Brain Model Export (same as before)\n",
    "\n",
    "This Python code will load your NIfTI anatomical data, generate a 3D brain model, and save it as an .obj file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pyvista as pv\n",
    "from skimage import measure\n",
    "\n",
    "# Load the NIfTI data (e.g., anatomical brain scan)\n",
    "nii_file = 'path_to_your_brain_data.nii'  # Change this to the path of your NIfTI file\n",
    "nii_img = nib.load(nii_file)\n",
    "data = nii_img.get_fdata()\n",
    "\n",
    "# Create a binary mask: threshold the data to get the brain's surface\n",
    "threshold = np.max(data) * 0.3  # You can adjust the threshold value\n",
    "binary_mask = data > threshold\n",
    "\n",
    "# Extract the surface (isosurface) from the binary mask\n",
    "verts, faces, _, _ = measure.marching_cubes(binary_mask, level=threshold)\n",
    "\n",
    "# Convert to a PyVista mesh\n",
    "mesh = pv.PolyData(verts, faces)\n",
    "\n",
    "# Optionally, save the 3D brain model to a file format like STL or OBJ\n",
    "mesh.save('brain_model.obj')  # Save as OBJ for 3D visualization in Three.js\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "source": [
    "#### Three.js code to visualize the brain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Brain Activity Differences Visualization</title>\n",
    "    <style>\n",
    "        body { margin: 0; overflow: hidden; }\n",
    "        canvas { display: block; }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js\"></script>\n",
    "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/OrbitControls.min.js\"></script>\n",
    "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/loaders/OBJLoader.js\"></script>\n",
    "\n",
    "<script>\n",
    "// Set up the 3D scene, camera, and renderer\n",
    "const scene = new THREE.Scene();\n",
    "const camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);\n",
    "const renderer = new THREE.WebGLRenderer();\n",
    "renderer.setSize(window.innerWidth, window.innerHeight);\n",
    "document.body.appendChild(renderer.domElement);\n",
    "\n",
    "// Load the brain model OBJ file\n",
    "const objLoader = new THREE.OBJLoader();\n",
    "objLoader.load('brain_model.obj', (object) => {\n",
    "    // Add the brain model to the scene\n",
    "    object.scale.set(2, 2, 2);  // Adjust scale if needed\n",
    "    scene.add(object);\n",
    "});\n",
    "\n",
    "// Set up the camera position\n",
    "camera.position.z = 20;\n",
    "\n",
    "// Add orbit controls to move around the scene\n",
    "const controls = new THREE.OrbitControls(camera, renderer.domElement);\n",
    "\n",
    "// Animation loop\n",
    "function animate() {\n",
    "    requestAnimationFrame(animate);\n",
    "    controls.update();  // Update controls (necessary for OrbitControls)\n",
    "    renderer.render(scene, camera);\n",
    "}\n",
    "animate();\n",
    "</script>\n",
    "\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "source": [
    "### Visualizing brains side-by-side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "source": [
    "Visualizing two anatomically correct brains in Three.js with the ability to compare regions, display differences in brain activity, and simulate the effects of electrical stimulation can be a complex task. Below is a step-by-step guide to implement this functionality, breaking it down into parts:\n",
    "1. Set Up Two Anatomically Correct Brains in Three.js\n",
    "\n",
    "    First, you'll need the 3D models for the brain anatomy (e.g., .obj, .stl, or .gltf format).\n",
    "    These models must have predefined regions for easy matching between the two brains.\n",
    "\n",
    "2. Create the Scene and Load Brain Models\n",
    "\n",
    "    We'll load two 3D models of brains and display them side-by-side in the same 3D scene.\n",
    "    Use THREE.js to load and display the models.\n",
    "\n",
    "3. Add Edges Between Matching Regions of the Two Brains\n",
    "\n",
    "    We'll create edges between matching regions of the two brains. The edge weights (colors) will be based on the difference in activity between the regions.\n",
    "\n",
    "4. Add Interactivity (Brain Selection)\n",
    "\n",
    "    Add UI elements to select which brain (on the left and right) to compare.\n",
    "    Use HTML or React for the interface, and update the visualization dynamically.\n",
    "\n",
    "5. Color the Edges Based on Activity Difference\n",
    "\n",
    "    The edges should change color based on the difference in activity (e.g., using a color gradient from green to red).\n",
    "\n",
    "6. Simulate Electrical Stimulation\n",
    "\n",
    "    For a specific brain region, simulate electrical stimulation by modifying the activity levels in that region.\n",
    "    Update the brain visualization in real-time to show how stimulation impacts the activity in the affected regions, restoring cognitive abilities.\n",
    "\n",
    "Example Code for Step 15:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Brain Comparison and Stimulation</title>\n",
    "    <style>\n",
    "        body { margin: 0; overflow: hidden; }\n",
    "        canvas { display: block; }\n",
    "        #controls { position: absolute; top: 20px; left: 20px; z-index: 10; color: white; }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div id=\"controls\">\n",
    "        <label for=\"brainSelectLeft\">Left Brain:</label>\n",
    "        <select id=\"brainSelectLeft\">\n",
    "            <option value=\"brain1\">Brain 1</option>\n",
    "            <option value=\"brain2\">Brain 2</option>\n",
    "        </select>\n",
    "\n",
    "        <label for=\"brainSelectRight\">Right Brain:</label>\n",
    "        <select id=\"brainSelectRight\">\n",
    "            <option value=\"brain3\">Brain 3</option>\n",
    "            <option value=\"brain4\">Brain 4</option>\n",
    "        </select>\n",
    "    </div>\n",
    "\n",
    "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js\"></script>\n",
    "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/OrbitControls.min.js\"></script>\n",
    "\n",
    "<script>\n",
    "// Create the 3D scene, camera, and renderer\n",
    "const scene = new THREE.Scene();\n",
    "const camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);\n",
    "const renderer = new THREE.WebGLRenderer();\n",
    "renderer.setSize(window.innerWidth, window.innerHeight);\n",
    "document.body.appendChild(renderer.domElement);\n",
    "\n",
    "// Load brain models (using placeholders for now)\n",
    "const brain1Mesh = new THREE.Mesh(new THREE.SphereGeometry(5, 32, 32), new THREE.MeshBasicMaterial({ color: 0x00ff00 }));\n",
    "const brain2Mesh = new THREE.Mesh(new THREE.SphereGeometry(5, 32, 32), new THREE.MeshBasicMaterial({ color: 0x0000ff }));\n",
    "\n",
    "brain1Mesh.position.set(-10, 0, 0);\n",
    "brain2Mesh.position.set(10, 0, 0);\n",
    "\n",
    "scene.add(brain1Mesh);\n",
    "scene.add(brain2Mesh);\n",
    "\n",
    "// Initialize OrbitControls\n",
    "const controls = new THREE.OrbitControls(camera, renderer.domElement);\n",
    "\n",
    "// Set camera position\n",
    "camera.position.z = 50;\n",
    "\n",
    "// Activity difference (for demonstration, using random values for edges)\n",
    "const regions = ['Region A', 'Region B', 'Region C'];\n",
    "const activityDifferences = {\n",
    "    'Region A': { 'Region A': 0, 'Region B': 0.5, 'Region C': 0.2 },\n",
    "    'Region B': { 'Region A': 0.5, 'Region B': 0, 'Region C': 0.3 },\n",
    "    'Region C': { 'Region A': 0.2, 'Region B': 0.3, 'Region C': 0 },\n",
    "};\n",
    "\n",
    "// Add edges between matching regions (edges will be colored based on activity difference)\n",
    "const edges = [];\n",
    "regions.forEach(region1 => {\n",
    "    regions.forEach(region2 => {\n",
    "        if (region1 !== region2) {\n",
    "            const activityDiff = activityDifferences[region1][region2];\n",
    "            const edgeColor = new THREE.Color(0xff0000).lerp(new THREE.Color(0x00ff00), 1 - activityDiff);\n",
    "            edges.push({ region1, region2, activityDiff, color: edgeColor });\n",
    "        }\n",
    "    });\n",
    "});\n",
    "\n",
    "// Create edges (lines) between brain regions\n",
    "edges.forEach(edge => {\n",
    "    const material = new THREE.LineBasicMaterial({ color: edge.color });\n",
    "    const geometry = new THREE.Geometry();\n",
    "    geometry.vertices.push(new THREE.Vector3(Math.random() * 5, Math.random() * 5, Math.random() * 5)); // Random position for demonstration\n",
    "    geometry.vertices.push(new THREE.Vector3(Math.random() * 5, Math.random() * 5, Math.random() * 5)); // Random position for demonstration\n",
    "\n",
    "    const line = new THREE.Line(geometry, material);\n",
    "    scene.add(line);\n",
    "});\n",
    "\n",
    "// Update function for UI interactivity\n",
    "document.getElementById('brainSelectLeft').addEventListener('change', updateScene);\n",
    "document.getElementById('brainSelectRight').addEventListener('change', updateScene);\n",
    "\n",
    "// Simulate brain stimulation\n",
    "function updateScene() {\n",
    "    // Logic to update the scene (e.g., adjust the brain model and edge colors)\n",
    "    // You can update the positions or properties of regions based on the selected brains\n",
    "    console.log('Updating scene based on selected brains');\n",
    "}\n",
    "\n",
    "// Animate the scene\n",
    "function animate() {\n",
    "    requestAnimationFrame(animate);\n",
    "\n",
    "    controls.update();\n",
    "    renderer.render(scene, camera);\n",
    "}\n",
    "\n",
    "animate();\n",
    "</script>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Brain Models:\n",
    "\n",
    "    Replace the placeholder brain models (SphereGeometry) with actual 3D brain models (e.g., .obj, .stl, .gltf). You can load these models with THREE.OBJLoader, THREE.GLTFLoader, etc.\n",
    "\n",
    "2. Comparing Brain Regions:\n",
    "\n",
    "    In this code, the activityDifferences object holds the activity differences between regions, represented as a value between 0 (no difference) and 1 (maximum difference). You can dynamically update this based on real data.\n",
    "    The edges are visualized as lines between regions, with color gradients reflecting the activity difference (from green to red).\n",
    "\n",
    "3. Simulate Brain Stimulation:\n",
    "\n",
    "    For each brain, select a region and simulate stimulation by increasing or decreasing the activity. After stimulation, the model should be updated to reflect the normalized activity in the selected region.\n",
    "\n",
    "4. Interactivity:\n",
    "\n",
    "    HTML dropdowns (brainSelectLeft and brainSelectRight) allow users to select which two brains they want to compare.\n",
    "    When the selection changes, the scene can be updated by changing the models and edge colors.\n",
    "\n",
    "5. Future Enhancements:\n",
    "\n",
    "    Better Layout: For real brain region positioning, use a layout algorithm (e.g., spherical or customized) or data from an anatomical atlas.\n",
    "    Simulation: Implement electrical stimulation by modifying activity levels of specific regions and updating the visualization.\n",
    "\n",
    "6. Simulating Stimulation:\n",
    "\n",
    "    Create a function to simulate electrical stimulation. For example, you could gradually increase the activity of a specific region and see how it affects the brain's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
